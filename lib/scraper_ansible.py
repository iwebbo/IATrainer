"""
Copyright (c) 2025 [A&E Coding]

Permission est accord√©e, gratuitement, √† toute personne obtenant une copie
 de ce logiciel et des fichiers de documentation associ√©s (le "IAtrainer.py"),
 de traiter le Logiciel sans restriction, y compris, sans s'y limiter, les droits
 d'utiliser, de copier, de modifier, de fusionner, de publier, de distribuer, de sous-licencier,
 et/ou de vendre des copies du Logiciel, et de permettre aux personnes √† qui
 le Logiciel est fourni de le faire, sous r√©serve des conditions suivantes :

Le texte ci-dessus et cette autorisation doivent √™tre inclus dans toutes les copies
 ou portions substantielles du Logiciel.

LE LOGICIEL EST FOURNI "TEL QUEL", SANS GARANTIE D'AUCUNE SORTE, EXPLICITE OU IMPLICITE,
Y COMPRIS MAIS SANS S'Y LIMITER, LES GARANTIES DE QUALIT√â MARCHANDE, D'AD√âQUATION
√Ä UN USAGE PARTICULIER ET D'ABSENCE DE CONTREFA√áON. EN AUCUN CAS LES AUTEURS OU TITULAIRES
DU COPYRIGHT NE POURRONT √äTRE TENUS RESPONSABLES DE TOUTE R√âCLAMATION, DOMMAGE OU AUTRE RESPONSABILIT√â,
QUE CE SOIT DANS UNE ACTION CONTRACTUELLE, D√âLICTUELLE OU AUTRE, D√âCOULANT DE,
OU EN RELATION AVEC LE LOGICIEL OU L'UTILISATION OU D'AUTRES INTERACTIONS AVEC LE LOGICIEL.
"""

import json
import requests
from bs4 import BeautifulSoup
from fpdf import FPDF
import time
from playwright.sync_api import sync_playwright
import sys
import os



# üîπ R√©cup√©ration du BASE_DIR depuis `main.py`
BASE_DIR = sys.argv[2] if len(sys.argv) > 1 else os.path.dirname(os.path.abspath(__file__))

# üîπ Correction des chemins relatifs en utilisant BASE_DIR
FONT_PATH = os.path.join(BASE_DIR, "pdf", "DejaVuSans.ttf")  # Police TTF
PDF_DIR = os.path.join(BASE_DIR, "pdf")  # Dossier PDF
JSON_FILE = os.path.join(BASE_DIR, "scrap", "scrapped_menu_links_doc.json")  # JSON
PDF_FILE = "Documentation_Site.pdf"


def scrape_page(url):
    """Scrape le contenu principal d'une page Ansible en ex√©cutant le JavaScript."""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url, timeout=60000)

        # Attendre que le contenu principal se charge
        page.wait_for_selector("div.document", timeout=20000)

        # R√©cup√©rer le HTML apr√®s rendu
        html_content = page.content()
        browser.close()

    soup = BeautifulSoup(html_content, "html.parser")

    # Extraire le bon √©l√©ment contenant le texte
    content = soup.select_one("div.document")
    if not content:
        print(f"‚ùå Impossible d'extraire le contenu de {url}")
        return ""

    return content.get_text(separator="\n", strip=True)

class PDF(FPDF):
    def header(self):
        self.set_font("DejaVu", size=16)
        self.cell(200, 10, "Documentation Ansible", ln=True, align="C")
        self.ln(10)

def save_to_pdf(content_list, output_file=PDF_FILE):
    """Sauvegarde le contenu dans un PDF en UTF-8."""
    pdf = PDF()
    pdf.set_auto_page_break(auto=True, margin=15)

    # Ajouter la police UTF-8
    pdf.add_font("DejaVu", "", "DejaVuSans.ttf", uni=True)

    for title, text in content_list:
        pdf.add_page()
        pdf.set_font("DejaVu", size=14)
        pdf.cell(200, 10, title, ln=True, align="C")
        pdf.ln(10)

        pdf.set_font("DejaVu", size=12)
        pdf.multi_cell(0, 10, text)

    pdf.output(output_file)
    print(f"‚úÖ PDF sauvegard√© : {output_file}")

def main():
    """Ex√©cute le scraping et la cr√©ation du PDF."""
    print("üìñ Lecture du fichier JSON contenant les liens...")
    with open(JSON_FILE, "r", encoding="utf-8") as f:
        links = json.load(f)

    if not links:
        print("‚ùå Aucun lien trouv√©, arr√™t du script.")
        return

    content_list = []

    for entry in links:
        title, url = entry["title"], entry["url"]
        print(f"üìÑ Scraping : {title}")
        text = scrape_page(url)
        if text:
            content_list.append((title, text))
        time.sleep(1)  # √âvite le bannissement

    print("üìÑ G√©n√©ration du PDF...")
    save_to_pdf(content_list)

if __name__ == "__main__":
    main()
