"""
Copyright (c) 2025 [A&E Coding]

Permission est accord√©e, gratuitement, √† toute personne obtenant une copie
 de ce logiciel et des fichiers de documentation associ√©s (le "IAtrainer.py"),
 de traiter le Logiciel sans restriction, y compris, sans s'y limiter, les droits
 d'utiliser, de copier, de modifier, de fusionner, de publier, de distribuer, de sous-licencier,
 et/ou de vendre des copies du Logiciel, et de permettre aux personnes √† qui
 le Logiciel est fourni de le faire, sous r√©serve des conditions suivantes :

Le texte ci-dessus et cette autorisation doivent √™tre inclus dans toutes les copies
 ou portions substantielles du Logiciel.

LE LOGICIEL EST FOURNI "TEL QUEL", SANS GARANTIE D'AUCUNE SORTE, EXPLICITE OU IMPLICITE,
Y COMPRIS MAIS SANS S'Y LIMITER, LES GARANTIES DE QUALIT√â MARCHANDE, D'AD√âQUATION
√Ä UN USAGE PARTICULIER ET D'ABSENCE DE CONTREFA√áON. EN AUCUN CAS LES AUTEURS OU TITULAIRES
DU COPYRIGHT NE POURRONT √äTRE TENUS RESPONSABLES DE TOUTE R√âCLAMATION, DOMMAGE OU AUTRE RESPONSABILIT√â,
QUE CE SOIT DANS UNE ACTION CONTRACTUELLE, D√âLICTUELLE OU AUTRE, D√âCOULANT DE,
OU EN RELATION AVEC LE LOGICIEL OU L'UTILISATION OU D'AUTRES INTERACTIONS AVEC LE LOGICIEL.
"""

"""
README:

pip install beautifulsoup4 requests playwright
playwright install

"""
from playwright.sync_api import sync_playwright
import json
import time
import sys
import os

# üîπ V√©rifier si un argument est fourni
if len(sys.argv) < 2:
    print("‚ùå Erreur : Aucun terme de recherche fourni. Ex√©cution : python docget_duck.py 'votre requ√™te'")
    sys.exit(1)

# üîπ R√©cup√©rer la requ√™te depuis l'argument
SEARCH_QUERY = sys.argv[1]
JSON_FILE = "lib\scrap\scraped_data_duck.json"
BASE_URL = "https://duckduckgo.com/?q="

def scrape_duckduckgo(query):
    results = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)  # üî• `False` pour voir la page
        page = browser.new_page()

        # üîπ Charger la page avec la recherche
        search_url = f"{BASE_URL}{query.replace(' ', '+')}&ia=web"
        print(f"üîç URL charg√©e : {search_url}")
        page.goto(search_url, timeout=60000)

        # üîπ Nouvelle attente bas√©e sur `h2 a` (titre des r√©sultats)
        page.wait_for_selector("h2 a", timeout=20000)

        # üîπ Extraire les r√©sultats
        posts = page.query_selector_all("article")

        print(f"‚úÖ {len(posts)} r√©sultats trouv√©s")  # üî• Debug

        for post in posts:
            title_element = post.query_selector("h2 a")
            link_element = post.query_selector("h2 a[href]")
            summary_element = post.query_selector(".result__snippet")

            if title_element and link_element:
                title = title_element.inner_text()
                link = link_element.get_attribute("href")  # ‚úÖ Correction du lien
                summary = summary_element.inner_text() if summary_element else ""

                results.append({"title": title, "link": link, "summary": summary})

        browser.close()

    return results

def scrape_duckduckgo2(query, max_results=50):
    results = []
    loaded_links = set()  # ‚úÖ Stocker les liens d√©j√† r√©cup√©r√©s pour √©viter les doublons

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)  # ‚úÖ Voir le navigateur en action
        page = browser.new_page()

        # üîπ Charger la page avec la recherche
        search_url = f"{BASE_URL}{query.replace(' ', '+')}&ia=web"
        print(f"üîç URL charg√©e : {search_url}")
        page.goto(search_url, timeout=60000)

        # ‚úÖ Attendre que les premiers r√©sultats apparaissent
        page.wait_for_selector("h2 a", timeout=20000)

        while len(results) < max_results:
            # üîπ R√©cup√©rer les r√©sultats visibles
            posts = page.query_selector_all("article")

            for post in posts:
                title_element = post.query_selector("h2 a")
                link_element = post.query_selector("h2 a[href]")
                summary_element = post.query_selector(".result__snippet")

                if title_element and link_element:
                    title = title_element.inner_text().strip()
                    link = link_element.get_attribute("href")
                    summary = summary_element.inner_text().strip() if summary_element else ""

                    # üîπ √âviter les doublons
                    if link not in loaded_links:
                        results.append({"title": title, "link": link, "summary": summary})
                        loaded_links.add(link)  # ‚úÖ Ajouter le lien au set pour √©viter la duplication

                # üîπ Stop si max atteint
                if len(results) >= max_results:
                    break

            # üîΩ **Scroll vers le bas pour charger les nouveaux r√©sultats**
            print("üîΩ Scroll vers le bas...")
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)  # ‚è≥ Pause pour laisser les r√©sultats charger

            # üîΩ **V√©rifier et cliquer sur le bouton "Plus de r√©sultats"**
            more_results_btn = page.query_selector("a:has-text('Plus de r√©sultats'), a:has-text('More results')")

            if more_results_btn:
                print("üîΩ Clic sur 'Plus de r√©sultats'...")
                more_results_btn.click()  # ‚úÖ Clic automatique
                time.sleep(3)  # ‚è≥ Pause pour permettre le chargement
            else:
                print("üö´ Aucun bouton 'Plus de r√©sultats' d√©tect√©.")
                break  # üî• Sortie de la boucle si pas de bouton

        browser.close()

    return results



# üîπ Sauvegarde en JSON
def save_to_json(data, filename=JSON_FILE):
    """Sauvegarde les donn√©es en JSON"""
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)  # ‚úÖ √âcriture en JSON format√©
    print(f"‚úÖ Donn√©es sauvegard√©es dans {filename}")

if __name__ == "__main__":
    scraped_data = scrape_duckduckgo2(SEARCH_QUERY)
    if scraped_data:
        save_to_json(scraped_data)
    else:
        print("‚ùå Aucun r√©sultat trouv√©.")
    
    # BASE_DIR = os.path.dirname(os.path.abspath(__file__)) 
    # DUCK_PATH = os.path.join(BASE_DIR, "scraper_duck2.py")

    # print("üìÑ Scrapping all link founds and G√©n√©ration du PDF...")
    # os.system(f'python "{DUCK_PATH}"')
    # print("‚úÖ Finish.")